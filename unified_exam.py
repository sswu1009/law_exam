# -*- coding: utf-8 -*-
"""
æ•´åˆç‰ˆé¡Œåº«è‡ªå‹•è™•ç†æµç¨‹ï¼š
1ï¸âƒ£ æ··åˆæ¨¡å¼ç« ç¯€å°æ‡‰ï¼ˆé—œéµå­— + LLaMA åˆ¤æ–·ï¼‰
2ï¸âƒ£ è‡ªå‹•ä¾ç« ç¯€æ‹†åˆ†é¡Œåº«ï¼ˆæ¯ç« ç¯€ä¸€å€‹å·¥ä½œè¡¨ï¼Œåªä¿ç•™é¡Œç›®ã€é¸é …ã€ç­”æ¡ˆï¼‰
"""

import pandas as pd
import requests, json
from docx import Document
from collections import OrderedDict, defaultdict
from tqdm import tqdm
import difflib
import os

# ==============================
# æª”æ¡ˆè¨­å®š
# ==============================
DOCX_PATH = "/Users/lch/Downloads/JYé›»å­æª”ç­†è¨˜-äººèº«ä¿éšª-(è§£å¯† - è¤‡è£½.docx"
EXCEL_PATH = "/Users/lch/lawbroker/é¡Œåº«/å¤–å¹£/é¡Œåº«_FCI_åˆ†ç« _202411.xlsx"
OUTPUT_PATH_MIXED = "FCI_ç« ç¯€å°æ‡‰çµæœ.xlsx"
OUTPUT_PATH_SPLIT = "FCI_é¡Œåº«_ä¾ç« ç¯€åˆ†é .xlsx"

QUESTION_COL = "é¡Œç›®"
OPTION_COLS = ["é¸é …ä¸€","é¸é …äºŒ","é¸é …ä¸‰","é¸é …å››"]
ANSWER_COL = "ç­”æ¡ˆ"
OUTPUT_COL = "ç« ç¯€å°æ‡‰(æ··åˆ)"
RAW_COL = "LLMåŸå§‹è¼¸å‡º"
TAG_COL = OUTPUT_COL

KEEP_COLS = [QUESTION_COL, "é¸é …ä¸€", "é¸é …äºŒ", "é¸é …ä¸‰", "é¸é …å››", ANSWER_COL]

# ==============================
# é—œéµå­—è¡¨
# ==============================
CHAPTER_KEYWORDS = OrderedDict({
    "ä¿éšªå¥‘ç´„": ["å¥‘ç´„","æ’¤éŠ·","å¯¬é™æœŸ","å‚¬å‘Š","åœæ•ˆ","å¾©æ•ˆ","ä¿å–®åƒ¹å€¼"],
    "ä¿éšªå¥‘ç´„å…­å¤§åŸå‰‡": ["æœ€å¤§å–„æ„","å¯ä¿åˆ©ç›Š","æå®³å¡«è£œ","åˆ†æ”¤","ä»£ä½","å‘ŠçŸ¥ç¾©å‹™"],
    "ä¿éšªé‡‘èˆ‡è§£ç´„é‡‘": ["è§£ç´„é‡‘","é€€ä¿","æ­»äº¡çµ¦ä»˜","æ»¿æœŸé‡‘","ç”Ÿå­˜çµ¦ä»˜","è‡ªæ®º","æ•…æ„"],
    "éºç”¢ç¨…èˆ‡è´ˆèˆ‡ç¨…": ["éºç”¢ç¨…","è´ˆèˆ‡ç¨…","å…ç¨…é¡","æ‰£é™¤é¡","èª²ç¨…","åˆ†æœŸ","ç”³å ±"],
    "å¥åº·ä¿éšª": ["å¥åº·ä¿éšª","é†«ç™‚éšª","å¯¦æ”¯å¯¦ä»˜","æ—¥é¡çµ¦ä»˜","ä½é™¢","æ‰‹è¡“","é‡å¤§ç–¾ç—…","ç™Œç—‡éšª"],
    "äººå£½ä¿éšª": ["çµ‚èº«å£½éšª","å®šæœŸå£½éšª","ç”Ÿæ­»åˆéšª","è®Šé¡å£½éšª","æŠ•è³‡å‹"],
    "å¹´é‡‘ä¿éšª": ["å¹´é‡‘","å³æœŸå¹´é‡‘","éå»¶å¹´é‡‘","ç”Ÿå­˜å¹´é‡‘","é€€ä¼‘è¦åŠƒ"],
    "å‚·å®³ä¿éšª": ["å‚·å®³ä¿éšª","æ„å¤–","å¤–ä¾†","çªç™¼","éç–¾ç—…","æ®˜å»¢","æ„å¤–é†«ç™‚"],
})
ALLOWED_CHAPTERS = list(CHAPTER_KEYWORDS.keys())

# ==============================
# èƒå–ç« ç¯€å…§å®¹
# ==============================
def extract_chapters(docx_path):
    doc = Document(docx_path)
    chapters = {}
    current = None
    for p in doc.paragraphs:
        t = p.text.strip()
        if not t:
            continue
        if t.startswith("ç¬¬") and "ç« " in t:
            current = t
            chapters[current] = []
        elif current:
            chapters[current].append(t)
    return {ch: " ".join(txts)[:600] for ch, txts in chapters.items()}

# ==============================
# LLaMA API
# ==============================
def ask_llama(prompt, model="llama3"):
    try:
        url = "http://localhost:11434/api/generate"
        payload = {"model": model, "prompt": prompt}
        resp = requests.post(url, json=payload, stream=True)
        output = ""
        for line in resp.iter_lines():
            if line:
                data = json.loads(line.decode("utf-8"))
                output += data.get("response", "")
        return output.strip()
    except Exception as e:
        print(f"âš ï¸ LLaMA å‘¼å«å¤±æ•—ï¼š{e}")
        return ""

def classify_with_llama(question_text, chapters):
    chapters_text = "ã€".join(ALLOWED_CHAPTERS)
    prompt = f"""
ä½ æ˜¯ä¸€å€‹ä¿éšªè€ƒç…§å°ˆå®¶ã€‚è«‹æ ¹æ“šé¡Œç›®èˆ‡æ•™æç« ç¯€å…§å®¹ï¼Œåˆ¤æ–·é¡Œç›®æ¶‰åŠå“ªäº›ç« ç¯€ã€‚

ã€è¦å‰‡ã€‘ï¼š
1. åªèƒ½å¾ä»¥ä¸‹ç« ç¯€åç¨±ä¸­é¸ï¼ŒåŸå°ä¸å‹•è¼¸å‡ºï¼š
{chapters_text}
2. å¯ä»¥é¸å¤šå€‹ç« ç¯€ï¼Œç”¨é€—è™Ÿåˆ†éš”ã€‚
3. ä¸è¦è¼¸å‡ºé¡å¤–è§£é‡‹ã€‚

é¡Œç›®ï¼š
{question_text}

è«‹ç›´æ¥è¼¸å‡ºç« ç¯€åç¨±ã€‚
"""
    raw_result = ask_llama(prompt)
    result_clean = []

    for part in raw_result.replace("ã€", ",").split(","):
        part = part.strip()
        if not part:
            continue
        best_match = difflib.get_close_matches(part, ALLOWED_CHAPTERS, n=1, cutoff=0.5)
        if best_match:
            result_clean.append(best_match[0])

    result_clean = list(OrderedDict.fromkeys(result_clean))
    return "ã€".join(result_clean), raw_result

# ==============================
# é—œéµå­—æ¯”å°
# ==============================
def keyword_match(text):
    hits = []
    for chapter, kws in CHAPTER_KEYWORDS.items():
        if any(kw in text for kw in kws):
            hits.append(chapter)
    return "ã€".join(OrderedDict.fromkeys(hits))

# ==============================
# (1) æ··åˆæ¨¡å¼ä¸»æµç¨‹
# ==============================
def process_excel(input_path, output_path, chapters):
    excel = pd.ExcelFile(input_path)
    writer = pd.ExcelWriter(output_path, engine="xlsxwriter")

    for sheet in excel.sheet_names:
        df = pd.read_excel(input_path, sheet_name=sheet)
        if QUESTION_COL not in df.columns:
            df.to_excel(writer, sheet_name=sheet, index=False)
            continue

        mapped, raw_outputs = [], []
        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"è™•ç† {sheet}"):
            q = str(row.get(QUESTION_COL, ""))
            opts = [str(row.get(c, "")) for c in OPTION_COLS if c in df.columns]
            text = q + " " + " ".join(opts)

            result = keyword_match(text)
            raw_answer = ""

            if not result:
                result, raw_answer = classify_with_llama(text, chapters)

            mapped.append(result)
            raw_outputs.append(raw_answer)

        df[OUTPUT_COL] = mapped
        df[RAW_COL] = raw_outputs
        df.to_excel(writer, sheet_name=sheet, index=False)

    writer.close()
    print(f"âœ… ç« ç¯€å°æ‡‰å®Œæˆï¼š{output_path}")

# ==============================
# (2) ä¾ç« ç¯€æ‹†åˆ†ç²¾ç°¡é¡Œåº«
# ==============================
def split_excel_by_chapter(input_path, output_path, tag_col=TAG_COL):
    excel = pd.ExcelFile(input_path)
    chapter_dfs = defaultdict(list)

    for sheet in excel.sheet_names:
        df = pd.read_excel(input_path, sheet_name=sheet)
        keep_cols_exist = [c for c in KEEP_COLS if c in df.columns]
        if not keep_cols_exist or tag_col not in df.columns:
            continue

        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"è®€å– {sheet}"):
            tags = str(row.get(tag_col, "")).strip()
            if not tags or tags == "nan":
                continue

            chapters = [t.strip() for t in tags.replace(",", "ã€").split("ã€") if t.strip()]
            clean_row = row[keep_cols_exist]

            for ch in chapters:
                chapter_dfs[ch].append(clean_row)

    with pd.ExcelWriter(output_path, engine="xlsxwriter") as writer:
        for ch in sorted(chapter_dfs.keys()):
            out_df = pd.DataFrame(chapter_dfs[ch])
            safe_name = ch.replace("/", "_").replace("\\", "_").replace("*", "_")[:28]
            out_df.to_excel(writer, sheet_name=safe_name, index=False)

    print(f"âœ… å·²ä¾ç« ç¯€åˆ†é è¼¸å‡ºç²¾ç°¡é¡Œåº«ï¼š{output_path}")
    print(f"ç¸½ç« ç¯€æ•¸ï¼š{len(chapter_dfs)}")

# ==============================
# ä¸»åŸ·è¡Œæµç¨‹
# ==============================
if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹è™•ç†é¡Œåº«ç« ç¯€å°æ‡‰...")
    chapters = extract_chapters(DOCX_PATH)
    process_excel(EXCEL_PATH, OUTPUT_PATH_MIXED, chapters)

    print("\nğŸ“˜ é–‹å§‹ä¾ç« ç¯€æ‹†åˆ†é¡Œåº«...")
    split_excel_by_chapter(OUTPUT_PATH_MIXED, OUTPUT_PATH_SPLIT)

    print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")
